import numpy as np
import streamlit as st
import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectKBest, f_classif, RFE, chi2
from sklearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
import matplotlib.pyplot as plt
from scipy.stats import spearmanr, chi2_contingency
import seaborn as sns
from mca import MCA  # Aseg√∫rate de tener instalada la librer√≠a: pip install mca
from sklearn.base import BaseEstimator, TransformerMixin
import prince
from sklearn.preprocessing import FunctionTransformer, LabelEncoder, StandardScaler
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA


st.set_page_config(page_title="Taller1_MLBioestad√≠stica", layout="wide")
st.title("An√°lisis, preprocesamiento y reducci√≥n de dimensionalidad Dry Eye Disease")

@st.cache_data
def load_data():
    url = "https://github.com/andrews06ml/Taller1_MLBioestad-stica/blob/main/Dry_Eye_Dataset.csv"
    df = pd.read_csv(url)
    df.columns = df.columns.str.strip()
    return df

df = load_data()

# Asignar condiciones
# def assign_condition(row):
#     if (row["BPXSY1"] >= 140 or row["BPXSY2"] >= 140 or 
#         row["BPXDI1"] >= 90 or row["BPXDI2"] >= 90):
#         return "hypertension"
#     elif row["BMXBMI"] >= 30:
#         return "diabetes"
#     elif ((row["RIAGENDR"] == 1 and row["BMXWAIST"] > 102) or 
#           (row["RIAGENDR"] == 2 and row["BMXWAIST"] > 88)):
#         return "high cholesterol"
#     else:
#         return "healthy"

# df["Condition"] = df.apply(assign_condition, axis=1)

st.markdown("""
# üè• **Taller No. 1 - Machine learning II para bioestad√≠stica**
---

**Integrantes:**
- *Andres Felipe Montenegro*
- *Samuel Forero Martinez*

## **Base de datos utilizada para el desarrollor del taller: *DRY EYE DISEASE*** üëÄ

Se trata de un conjunto de datos amplio pensado para modelos predictivos y an√°lisis diagn√≥stico de la Enfermedad del Ojo Seco (DED), usando variables como calidad y duraci√≥n del sue√±o, enrojecimiento ocular, picaz√≥n, tiempo frente a pantallas, uso de filtro de luz azul y fatiga ocular.

Incluye datos estructurados de personas entre 18 y 45 a√±os, permitiendo investigar la relaci√≥n entre h√°bitos de vida y salud ocular. Puede aplicarse en machine learning, an√°lisis estad√≠stico y decisiones cl√≠nicas para mejorar la detecci√≥n temprana y tratamientos personalizados, e incluso predecir enfermedades relacionadas con el sue√±o como el insomnio, que puede estar vinculado a enfermedades de la superficie ocular.

El archivo contiene informaci√≥n de unas 20.000 personas (adolescentes, adultos de mediana edad y mayores) de ambos sexos. Incluye columnas como pasos diarios, tiempo de sue√±o, pulso, presi√≥n arterial, h√°bitos de alimentaci√≥n y bebida, niveles de estr√©s, problemas m√©dicos (ansiedad, hipertensi√≥n, asma, etc.) y medicamentos utilizados, adem√°s de atributos oculares b√°sicos para predecir la DED. Est√° en formato CSV y puede aportar a diversos tipos de investigaci√≥n m√©dica relacionada con la salud ocular y los h√°bitos de las personas.

 ***Link con mayor informaci√≥n: [Enfermedad del Ojo seco](https://www.kaggle.com/datasets/dakshnagra/dry-eye-disease)***

### Prop√≥sito del An√°lisis en esta App:

- Clasificar a los participantes en grupos de salud: **hipertensi√≥n**, **diabetes**, **alto colesterol** y **saludable**, basados en indicadores cl√≠nicos.
- Realizar un an√°lisis exploratorio con t√©cnicas de reducci√≥n de dimensionalidad como:
  - PCA (An√°lisis de Componentes Principales) para variables num√©ricas.
  - MCA (An√°lisis de Correspondencias M√∫ltiples) para variables categ√≥ricas.
- Seleccionar las variables m√°s relevantes para la clasificaci√≥n usando t√©cnicas estad√≠sticas y de machine learning.

### Importancia:

NHANES es un recurso valioso para investigadores, m√©dicos y pol√≠ticas p√∫blicas que buscan entender factores de riesgo y prevalencia de enfermedades cr√≥nicas en la poblaci√≥n estadounidense. Este an√°lisis ayuda a identificar patrones clave en los datos que pueden guiar intervenciones de salud.

---

**Fuente:** [NHANES 2015-2016](https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/default.aspx?BeginYear=2015)
""")


# Mostrar info y variables categ√≥ricas lado a lado
st.subheader("Resumen de Datos")

# Crear columnas para mostrar info_df y category_df lado a lado
col1, col2 = st.columns(2)

with col1:
    st.markdown("**Tipo de Dato y Nulos**")
    info_df = pd.DataFrame({
        "Column": df.columns,
        "Non-Null Count": df.notnull().sum().values,
        "Dtype": df.dtypes.values
    })
    st.dataframe(info_df, use_container_width=True)

# Detecci√≥n autom√°tica de variables categ√≥ricas
categorical_vars = [col for col in df.columns 
                    if df[col].dtype == 'object' or 
                       df[col].dtype == 'string' or 
                       df[col].nunique() <= 10]

with col2:
    st.markdown("**Variables Categ√≥ricas Detectadas**")
    category_info = []
    for col in categorical_vars:
        unique_vals = df[col].dropna().unique()
        category_info.append({
            "Variable": col,
            "Unique Classes": ", ".join(map(str, sorted(unique_vals)))
        })

    category_df = pd.DataFrame(category_info)
    st.dataframe(category_df, use_container_width=True)


st.subheader("Primeras 10 filas del dataset")
st.dataframe(df.head(10), use_container_width=True)

# Filtros
with st.sidebar:
    st.header("Filters")
    gender_filter = st.multiselect("Gender", sorted(df["Gender"].dropna().unique()))
    race_filter = st.multiselect("Race/Ethnicity", sorted(df["Race/Ethnicity"].dropna().unique()))
    condition_filter = st.multiselect("Condition", sorted(df["Condition"].dropna().unique()))
    #st.markdown("---")
    #k_vars = st.slider("Number of variables to select", 2, 10, 5)

# Aplicar filtros
for col, values in {
    "Gender": gender_filter, "Race/Ethnicity": race_filter, "Condition": condition_filter
}.items():
    if values:
        df = df[df[col].isin(values)]

if df.empty:
    st.warning("No data available after applying filters.")
    st.stop()

# Mostrar advertencias
problematic_cols = df.columns[df.dtypes == "object"].tolist()
nullable_ints = df.columns[df.dtypes.astype(str).str.contains("Int64")].tolist()

st.write("### ‚ö†Ô∏è Columnas potencialmente problem√°ticas para Arrow/Streamlit:")
if problematic_cols or nullable_ints:
    st.write("**Tipo 'object':**", problematic_cols)
    st.write("**Tipo 'Int64' (nullable):**", nullable_ints)
else:
    st.success("‚úÖ No hay columnas problem√°ticas detectadas.")

# Separar variables
#target_col = "Condition"

# Evitar que la variable objetivo quede en X
#X = df.drop(columns=[target_col])
y = df["Condition"]


numeric_features = df.select_dtypes(include=["float64", "int64"]).columns.tolist()
if 'Condition' in numeric_features:
    numeric_features.remove('Condition')

X_num = df[numeric_features]
X_cat = df[categorical_vars]

st.markdown("""
## ¬øQu√© es el An√°lisis de Componentes Principales (PCA)?

El **An√°lisis de Componentes Principales (PCA)** es una t√©cnica estad√≠stica usada para reducir la dimensionalidad de grandes conjuntos de datos con muchas variables num√©ricas, manteniendo la mayor cantidad posible de la variabilidad original.

### ¬øC√≥mo funciona?

- PCA transforma las variables originales en un nuevo conjunto de variables no correlacionadas llamadas **componentes principales**.
- Cada componente principal es una combinaci√≥n lineal de las variables originales.
- Los primeros componentes capturan la mayor parte de la variabilidad en los datos.
- Esto permite visualizar y analizar datos complejos en menos dimensiones, facilitando la interpretaci√≥n.

### ¬øPor qu√© usamos PCA en este an√°lisis?

- Para explorar patrones y agrupamientos en las variables num√©ricas del dataset NHANES.
- Para visualizar relaciones entre individuos y variables de forma simplificada.
- Para identificar cu√°les variables contribuyen m√°s a la variabilidad total.

---
""")

# Pipeline con imputaci√≥n, escalado y PCA
numeric_pipeline = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler()),
    ("pca", PCA(n_components=10))
])
X_num_pca = numeric_pipeline.fit_transform(X_num)

# DataFrame con componentes
pca_df = pd.DataFrame(X_num_pca, columns=[f"PC{i+1}" for i in range(10)])
pca_df["Condition"] = y.values

# Extraer el objeto PCA del pipeline
pca = numeric_pipeline.named_steps["pca"]

# Obtener la varianza explicada de cada componente
explained_variance_ratio = pca.explained_variance_ratio_

# Calcular la varianza acumulada
cumulative_variance = np.cumsum(explained_variance_ratio)

# Graficar
fig, ax = plt.subplots(figsize=(8,5))
ax.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')
ax.set_xlabel("N√∫mero de Componentes Principales")
ax.set_ylabel("Varianza Acumulada")
ax.set_title("Varianza Acumulada de las Componentes Principales")
ax.set_xticks(range(1, len(cumulative_variance) + 1))
ax.grid(True)
st.pyplot(fig)

# Gr√°fico PCA PC1 vs PC2
st.subheader("PCA - PC1 vs PC2")
fig, ax = plt.subplots(figsize=(8, 6))
sns.scatterplot(data=pca_df, x="PC1", y="PC2", hue="Condition", palette="Set2", alpha=0.8, ax=ax)
ax.set_title("PCA - PC1 vs PC2")
ax.set_xlabel("PC1")
ax.set_ylabel("PC2")
st.pyplot(fig)

st.markdown("""
### ¬øQu√© son los Loadings en PCA?

En el An√°lisis de Componentes Principales (PCA), los **loadings** (o cargas factoriales) representan la relaci√≥n entre las variables originales y las componentes principales.

---

### C√≥mo se calculan

PCA busca nuevas variables llamadas **componentes principales**, que son combinaciones lineales de las variables originales.

Cada componente principal $PC_j$ se define como:

$$
PC_j = a_{1j} x_1 + a_{2j} x_2 + \cdots + a_{pj} x_p
$$

donde:

- $x_1, x_2, \ldots, x_p$ son las variables originales (normalizadas si se aplica escalado),
- $a_{ij}$ son los coeficientes o **loadings** de la variable $i$ en la componente $j$.

---

### Interpretaci√≥n

- El loading $a_{ij}$ indica **cu√°nto aporta** la variable $i$ a la componente principal $j$.
- Valores altos (en valor absoluto) significan que esa variable influye fuertemente en la componente.
- El signo indica la direcci√≥n de la relaci√≥n (positiva o negativa).

---

### C√°lculo pr√°ctico

- Los loadings corresponden a los **autovectores** (vectores propios) de la matriz de covarianza o correlaci√≥n de los datos.
- En `scikit-learn`, el atributo `components_` del objeto PCA contiene estos loadings:  
  - Cada fila es una componente principal,  
  - Cada columna es una variable original.

---

### Usos

- Analizar los loadings ayuda a interpretar qu√© variables definen cada componente.
- Tambi√©n es √∫til para seleccionar variables importantes seg√∫n su contribuci√≥n.
""")

# Obtener los loadings del PCA (componentes * caracter√≠sticas)
loadings = numeric_pipeline.named_steps["pca"].components_

# Convertir a DataFrame con nombres de columnas
loadings_df = pd.DataFrame(
    loadings,
    columns=X_num.columns,
    index=[f"PC{i+1}" for i in range(loadings.shape[0])]
).T  # Transponer para que columnas sean PCs y filas las variables

# Ordenar las filas por la importancia de la variable en la suma de cuadrados de los componentes
# Esto agrupa por aquellas variables con mayor contribuci√≥n total
loading_magnitude = (loadings_df**2).sum(axis=1)
loadings_df["Importance"] = loading_magnitude
loadings_df_sorted = loadings_df.sort_values(by="Importance", ascending=False).drop(columns="Importance")

# Graficar heatmap ordenado
st.subheader("üîç Heatmap de Loadings del PCA (Componentes Principales)")

fig, ax = plt.subplots(figsize=(10, 12))
sns.heatmap(loadings_df_sorted, annot=True, cmap="coolwarm", center=0, ax=ax)
st.pyplot(fig)

st.markdown("""
### ¬øQu√© es el An√°lisis de Correspondencias M√∫ltiples (MCA)?

El An√°lisis de Correspondencias M√∫ltiples (MCA) es una t√©cnica estad√≠stica exploratoria utilizada para analizar y visualizar datos categ√≥ricos. Es una extensi√≥n del An√°lisis de Correspondencias Simple (CA) cuando hay m√°s de dos variables categ√≥ricas.

**Objetivos principales del MCA:**

- **Reducir la dimensionalidad** de datos categ√≥ricos complejos.
- **Identificar patrones** y relaciones entre categor√≠as de variables.
- **Visualizar** asociaciones entre individuos y categor√≠as en un espacio de menor dimensi√≥n.

**¬øC√≥mo funciona?**

El MCA transforma las variables categ√≥ricas en un espacio num√©rico, similar al An√°lisis de Componentes Principales (PCA) para variables num√©ricas. Luego, representa las observaciones y categor√≠as en un mapa factorial bidimensional o tridimensional, donde la proximidad entre puntos indica similitudes.

**Aplicaciones comunes:**

- Encuestas y estudios sociales con muchas variables categ√≥ricas.
- An√°lisis de perfiles de consumidores.
- Estudios epidemiol√≥gicos para agrupar caracter√≠sticas cl√≠nicas o sociodemogr√°ficas.

En resumen, MCA es una herramienta poderosa para explorar y resumir grandes conjuntos de datos categ√≥ricos y facilita la interpretaci√≥n visual de relaciones complejas.
""")

# Pipeline para MCA con prince
categorical_pipeline = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("encoder", OneHotEncoder(handle_unknown="ignore", sparse=False))
])

# Transformar datos categ√≥ricos
X_cat_encoded = categorical_pipeline.fit_transform(X_cat)

# Crear DataFrame con nombres de columnas despu√©s del one-hot encoding
encoded_columns = categorical_pipeline.named_steps["encoder"].get_feature_names_out(X_cat.columns)
X_cat_encoded_df = pd.DataFrame(X_cat_encoded, columns=encoded_columns, index=X_cat.index)

# Aplicar MCA con prince
mca = prince.MCA(n_components=2, random_state=42)
X_cat_mca = mca.fit_transform(X_cat_encoded_df)

# Agregar columna de condici√≥n para colorear
mca_df = X_cat_mca.copy()
mca_df["Condition"] = y.values
mca_df.columns = ["Dim1", "Dim2", "Condition"]

st.subheader("MCA - Dim1 vs Dim2")
fig, ax = plt.subplots(figsize=(8, 6))
sns.scatterplot(data=mca_df, x="Dim1", y="Dim2", hue="Condition", palette="Set1", alpha=0.8, ax=ax)
ax.set_title("MCA - Dim1 vs Dim2")
ax.set_xlabel("Dim1")
ax.set_ylabel("Dim2")
st.pyplot(fig)

# ======================
# HEATMAP DE CONTRIBUCIONES EN MCA
# ======================


st.subheader("üîç Contribuciones de las Variables Categ√≥ricas al MCA")

# Obtener contribuciones a las dimensiones
contribs = mca.column_contributions_

#st.write("Column names in contributions DataFrame:", contribs.columns.tolist())

# Seleccionar contribuciones a Dim1 y Dim2
contribs_selected = contribs[[0, 1]]  # 0 = Dim1, 1 = Dim2
contribs_selected.columns = ["Dim1", "Dim2"]

# Ordenar por Dim1 para mejor visualizaci√≥n (opcional)
#contribs_sorted = contribs_selected.sort_values(by=0, ascending=False)
contribs_sorted = contribs_selected.sort_values("Dim1", ascending=False)

# Crear heatmap
fig, ax = plt.subplots(figsize=(10, max(6, 0.3 * len(contribs_sorted))))
sns.heatmap(contribs_sorted, cmap="YlGnBu", annot=True, fmt=".2f", ax=ax)
ax.set_title("Contribuciones de las Variables a las Dimensiones del MCA")
st.pyplot(fig)


# ======================
# üîß Construir X
# ======================

# Separar por tipo
x_num = df.select_dtypes(include=["float64", "int64"]).drop(columns=["SEQN"], errors="ignore")
x_cat = df.select_dtypes(include=["object", "category", "bool", "string"])

# Guardar nombres originales
num_features = x_num.columns.tolist()
cat_features = x_cat.columns.tolist()

# Preprocesadores
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="mean")),
    ("scaler", StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("encoder", OneHotEncoder(handle_unknown="ignore", sparse_output=False))
])

# ColumnTransformer combinado
preprocessor = ColumnTransformer(transformers=[
    ("num", numeric_transformer, num_features),
    ("cat", categorical_transformer, cat_features)
])

# Ajustar y transformar
preprocessor.fit(df)

# Obtener nombres despu√©s del preprocesamiento
cat_encoded_columns = preprocessor.named_transformers_["cat"].named_steps["encoder"].get_feature_names_out(cat_features)
feature_names = np.concatenate([num_features, cat_encoded_columns])

# Transformar el dataset
X = preprocessor.transform(df)
X_df = pd.DataFrame(X, columns=feature_names)

# ======================
# üîç Selecci√≥n de Variables
# ======================
st.header("üîç Selecci√≥n de Variables")

# Variable objetivo
y = df["Condition"]

# Train/Test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X_df, y, test_size=0.2, stratify=y, random_state=42
)

# ============================
# 1Ô∏è‚É£ Selecci√≥n basada en modelos
# ============================
with st.expander("1Ô∏è‚É£ Selecci√≥n basada en modelos (Random Forest)"):
    model = RandomForestClassifier(random_state=42)
    model.fit(X_train, y_train)
    importances = pd.Series(model.feature_importances_, index=feature_names).sort_values(ascending=False)

    st.subheader("Importancia de variables")
    fig, ax = plt.subplots(figsize=(8, 5))
    sns.barplot(x=importances.head(7), y=importances.head(7).index, ax=ax)
    st.pyplot(fig)

    st.write("Precisi√≥n en test:", np.round(model.score(X_test, y_test), 3))

# ============================
# 3Ô∏è‚É£ Selecci√≥n por envoltura
# ============================
with st.expander("3Ô∏è‚É£ Selecci√≥n por envoltura (RFE con Regresi√≥n Log√≠stica)"):
    logistic = LogisticRegression(max_iter=500, solver='liblinear')
    rfe = RFE(estimator=logistic, n_features_to_select=7)
    rfe.fit(X_train, y_train)
    selected_features = X_train.columns[rfe.support_]

    st.write("Variables seleccionadas:", selected_features.tolist())

    coefs = pd.Series(logistic.fit(X_train[selected_features], y_train).coef_[0], index=selected_features)
    fig, ax = plt.subplots(figsize=(8, 5))
    sns.barplot(x=coefs, y=coefs.index, ax=ax)
    st.pyplot(fig)
    
















